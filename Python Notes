File used use an FYI for any handy Python notes

good link for refreashers (https://pandas.pydata.org/pandas-docs/version/0.22.0/10min.html)

online data
-> From pandas_datareader.data import DataReader
-> From date time import date
-> DataReader(Series_code, data_cource, start)

Don't forget about pandas concat()

How to read an excel sheet in pandas?
-> nyse = pd.read_excel('listings.xlsx', sheetname='nyse', na_values='n/a')
-> nyse = nyse.sort_values('Market Capitalization', ascending=False)

Plotting tips
-secondary_y attribute for plot will display a column on the right axis with a different scale
-plt.tight_layout() <- reduces the whitespace

.loc[row_selector, column_selector]

.info() gives you a quick summary

Lists
-Indexed by range of numbers
-Keys in dictionaries are immmutable objects

Dictionaries
-Indexed by unique keys
-Lookup table with unique keys

Numpy arrays can only contain values that are the same datatype

In Pandas 
  -Selecting Data
    -loc is label-based
    -iloc is interger position based
    -you can combine loc & iloc using ix
  -The single bracket version gives a Pandas Series, the double bracket version gives a Pandas DataFrame.
  - DF.Column.nunique() method allows to find how many unique values the column has
  - In order to get number for unique values for each column do this
    -df.apply(lambda x: x.nunique())
    	-here apply is a call function on each column
	-lambda is an "anonymous function" receiveing each column as argument x
  -df.column.value_counts() gives you the count of each unique value
  	e.g. S&P_500_Index['IPO Year'].value_counts() will easily tell me when most IPOs occured and what year
	e.g. S&P_500_Index['IPO Year'].astype(int).value_counts() will clean up my years into integer values
  -by default Pandas will try to drop rows so if we want to drop column you need to specify axis=1 in the
   drop method()
  - Exchange.groupby('Sector) is helpful 
	
  

Remember that for string comparison, Python determines the relationship based on alphabetical order.

For NumpPy arrays you need to use logical_and(), logical_or(), and logical_not() for array comparisions 

Avoid while loops if need be, best use is for the below
  -repeating action until condition is met 

multi-layered numpy arrays need the nditer() method in order for looping  


inplace=true attribute in drop() method helps us save memory and avoid creating a new DF
tight_layout() method is super helpful
always inspect your data using .info() or similar method 

distplot() method under seaborn module is helpful!! 
•	the distplot() function in the seaborn package creates a histogram, where data is grouped into ranges and and plotted as bars, and fits a kernel density estimation (KDE), or smoothed histogram. 
•	You can use distplot() to create another kind of graph called a rugplot, which adds markers at the bottom of the chart to indicate the density of observations along the x axis.
•	Axvline() method will create  vertical line 
	E.g. seaborn.displot(data).axvline(data[‘number’].median(), color=’black’, ls=’---‘)
seaborn.distplot(a, bins=None, hist=True, kde=True, rug=False, ...)


pandas offers .dropna() & .fillna() methods 
use .describe() to check if there a skew in the data between  the mean and 50%, basically checking for fat tails or such

https://fred.stlouisfed.org/ for downloading free online economic data


